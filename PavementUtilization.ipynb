{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over view taken from Voyage challenge:\n",
    "    \n",
    "Overview: Our cities have been optimized for cars. \n",
    "City infrastructure was designed and built decades ago with the premise that we would all own and drive single \n",
    "occupancy vehicles that would be used about 5% of the day. \n",
    "This has led to wider and wider lanes in the name of safety, \n",
    "more and more roadways to accommodate more drivers, and an explosion of parking to \n",
    "accommodate these cars when they are idle. In a world with autonomous vehicles providing transportation-as-a-service, \n",
    "much of this existing infrastructure may no longer be necessary. As more people utilize rideshare, \n",
    "there’s much less need for parking. As vehicles become safer and can better detect other vehicles, \n",
    "lanes could be narrower again.\n",
    "By re-claiming this “excess” pavement, \n",
    "we can make better use of valuable land by building more parks, work space, housing, etc.\n",
    "\n",
    "\n",
    "Goal: Create a model of approximate land utilization loss from excessive pavement in urban and suburban regions. \n",
    "We would like to see the result in square miles gained back from unused roadway/parking and the value that can be unlocked \n",
    "with gaining that land back. We would like to be able to get results from any city/region!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources used:\n",
    "\n",
    "Google map\n",
    "Google earth pro\n",
    "Bassis semantic segmentation framework https://github.com/CosmiQ/basiss\n",
    "Parking lot location mark from https://twitter.com/reginaclewlow/status/1048988323870429184?s=21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was not able to complete the programming required for the estimation of potential pavement reutilization or cost saving \n",
    "at this time but I will explain plans to proceed further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to be answered:\n",
    "    \n",
    "What percentage of San Francisco, in square miles, is parking?\n",
    "\n",
    "My attempt at answering:\n",
    "\n",
    "I have thought about this question and initially I believe a bounding box estimate would suffice but soon realize that parking\n",
    "lots are not fully square nor are they guaranteed to be clear of buildings that take away from their overall area as well often\n",
    "times they do not face perpendicular to north south east or west. I look back to resources available and noticed that the \n",
    "twitter feed was a very good potential source for working with semantic segmentation. I initially thought of directly infer \n",
    "this answer from the tweeter image but a great deal of the image was blocked off by legends.\n",
    "\n",
    "![alt text](ConceptAssets/SanfranciscoParkingGarageheatMap.jpg \"Sanfrancisco area parkinglots\")\n",
    "\n",
    "Overviewing the map it appears that area between Palo Alto and San Jose is heavily clustered with parking as well as having fair amount of ruralesque locations\n",
    "tapering towards the top and bottom of the map. My initial attempt at gathering data was through google map but I soon realized \n",
    "that data gathering through google maps by snipping/screen capturing tools would be a slow and endeaverous process and switched \n",
    "to using Google earth pro and captured the area in 5-6 prints. Now that I have the data my thought was to use the Bassis \n",
    "semantic segmentation framework to train and classify map images. Which I soon realize that most of the framework was reusable \n",
    "without much change. I had to make changes to the calls to resize as it no longer scales (h, w) but instead (w, h) after some \n",
    "version update. \n",
    "\n",
    "I've made a few training attempts and noticed that the framework was very efficient at learning to predict parking lots that \n",
    "validatition loss no longer decreases after 1 epoch. Though the training loss decreases I feel that the model is overfitting. \n",
    "After all the most flexable model that works well under most scenario is much better than a perfect model that works only for \n",
    "training set.\n",
    "\n",
    "![alt text](ConceptAssets/PotentialOverFit.PNG \"Potentially overfitting\")\n",
    "\n",
    "Given the complete model one should beable to infer the amount of parking lot utilization by sanfrancisco by\n",
    "\n",
    "1. Install components needed to run Bassis ref https://github.com/CosmiQ/basiss\n",
    "2. Start bassis docker container\n",
    "3. Clone this repo\n",
    "4. Navigate to the root of the repo\n",
    "5. Create massive_file_list.csv for Sanfrancisco map based on example in repo\n",
    "6. Get the files placed in the folders specified in massive_file_list.csv\n",
    "7. Run command \n",
    " nohup python -u src/basiss.py \\\n",
    " \t--path ./ \\\n",
    " \t--model unet \\\n",
    " \t--mode test \\\n",
    " \t--file_list massive_file_list.csv \\\n",
    " \t--model_weights results/pavementCV_model_best.hdf5 \\\n",
    " \t--slice_x 400 --slice_y 400 \\\n",
    " \t--stride_x 300 --stride_y 300 \\\n",
    " \t--n_bands 3 \\\n",
    " \t--n_classes 2 \\\n",
    " \t--batchsize 16 \\\n",
    " \t--gpu 3 \\\n",
    " \t--prefix outputlog > \\\n",
    " \t\tresults/outputlog.log & tail -f results/outputlog.log\n",
    "8. Run a sum for 1s on the resulting mask file\n",
    "9. Multiply estimated meter per square based on the resolution of your image\n",
    "10. Search in google meter square to mile square and get a conversion and now we have miles squared utilization estimate for SF!\n",
    "\n",
    "If Palo Alto moved to 100% autonomous cars, what percentage of roadway in Palo Alto, in square miles, could be used for other purposes?\n",
    "\n",
    "If we were to base autonomous model of http://www.wsp-pb.com/Globaln/UK/WSPPB-Farrells-AV-whitepaper.pdf I believe the road way areas would reduce by a little more than half given autonomous vehicles can be much smaller and cater to city travel for 1-2 users and can safely manuver where by saving some of the additional buffer given to human operated vehicles. A mile square estimate could be obtained by using the bassis classification model mentioned in https://medium.com/the-downlinq/broad-area-satellite-imagery-semantic-segmentation-basiss-4a7ea2c8466f for classifying road routes and applying estimate average of road width multiplier of\n",
    " the routes then multiply by estimate of meter squared by image resolution used then multiply by 0.8/2.\n",
    " \n",
    "If the average size of a vehicle was halved, what percentage of roadway could be eliminated?\n",
    "\n",
    "About half potentially a little more given some of the extra width given to the road is for turning radius.\n",
    "\n",
    "\n",
    "If 50% of the vehicles in Portland were autonomous, what percentage of the current pavement footprint could be repurposed?\n",
    "\n",
    "It really depends on how the vehicles are distributed in the worst case where there is a perfect mix between autonomous and \n",
    "human operated vehicles all road ways must still be catered for human driven vehicles but parking lots can be freed up except for few servicing stations if we were to go with model where autonomous vehicles acts as driverless taxis. Suppose that 90% of vehicles are out on the field at all time then this can be estimated by repeating the same step for Sanfrancisco but now using satellite image of Portland which will give you square miles of parking lot areas then multiply this by (1/2*9/10) which should give an estimate for if portland was 50% autonomous without special regulation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
